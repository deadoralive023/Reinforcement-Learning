{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dynamic-programming.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPbiHLfePZpsaPbUaVzGUzX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deadoralive023/Reinforcement-Learning/blob/main/dynamic_programming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KncniB6_mjoE",
        "outputId": "16dea7f7-2e23-4b54-fef0-478aa794ef26"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager\n",
        "name = 'FrozenLake-v0'\n",
        "env = gym.make(name, is_slippery = False)\n",
        "\n",
        "#lets examine it\n",
        "print('Action Space:      ' + str(env.action_space))\n",
        "print('Reward Range:      ' + str(env.reward_range))\n",
        "print('Observation Space: ' + str(env.observation_space))\n",
        "env.render()\n",
        "\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Action Space:      Discrete(4)\n",
            "Reward Range:      (0, 1)\n",
            "Observation Space: Discrete(16)\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etnKJ9_5shLo"
      },
      "source": [
        "def plot(V,policy,col_ramp=1,dpi=175,draw_vals=False):\n",
        "    plt.rcParams['figure.dpi'] = dpi\n",
        "    plt.rcParams.update({'axes.edgecolor': (0.32,0.36,0.38)})\n",
        "    plt.rcParams.update({'font.size': 4 if env.env.nrow == 8 else 7})\n",
        "    plt.figure(figsize=(3,3))\n",
        "    plt.imshow(1-V.reshape(env.env.nrow,env.env.ncol)**col_ramp, cmap='gray', interpolation='none', clim=(0,1))\n",
        "    ax = plt.gca()\n",
        "    ax.set_xticks(np.arange(env.env.ncol)-.5)\n",
        "    ax.set_yticks(np.arange(env.env.nrow)-.5)\n",
        "    ax.set_xticklabels([])\n",
        "    ax.set_yticklabels([])\n",
        "    for s in range(env.nS):\n",
        "        x = s%env.env.nrow\n",
        "        y = int(s/env.env.ncol)\n",
        "        a = policy[s]\n",
        "        gray = np.array((0.32,0.36,0.38))\n",
        "        if env.desc.tolist()[y][x] == b'G': \n",
        "            plt.text(x-0.45,y-0.3, 'goal', color=(0.75,0.22,0.17), fontname='OpenSans', weight='bold')\n",
        "            continue\n",
        "        if a[0] > 0.0: plt.arrow(x, y, float(a[0])*-.84, 0.0, color=gray+0.2*(1-V[s]), head_width=0.1, head_length=0.1) # left\n",
        "        if a[1] > 0.0: plt.arrow(x, y, 0.0, float(a[1])*.84,  color=gray+0.2*(1-V[s]), head_width=0.1, head_length=0.1) # down\n",
        "        if a[2] > 0.0: plt.arrow(x, y, float(a[2])*.84, 0.0,  color=gray+0.2*(1-V[s]), head_width=0.1, head_length=0.1) # right\n",
        "        if a[3] > 0.0: plt.arrow(x, y, 0.0, float(a[3])*-.84, color=gray+0.2*(1-V[s]), head_width=0.1, head_length=0.1) # up\n",
        "        if env.desc.tolist()[y][x] == b'F': plt.text(x-0.45,y-0.3, 'ice', color=(gray*V[s]), fontname='OpenSans')\n",
        "        if env.desc.tolist()[y][x] == b'S': plt.text(x-0.45,y-0.3, 'start',color=(0.21,0.51,0.48), fontname='OpenSans', weight='bold')\n",
        "        if draw_vals and V[s]>0:\n",
        "            vstr = '{0:.1e}'.format(V[s]) if env.env.nrow == 8 else '{0:.6f}'.format(V[s])\n",
        "            plt.text(x-0.45,y+0.45, vstr, color=(gray*V[s]), fontname='OpenSans')\n",
        "    plt.grid(color=(0.42,0.46,0.48), linestyle=':')\n",
        "    ax.set_axisbelow(True)\n",
        "    ax.tick_params(color=(0.42,0.46,0.48),which='both',top='off',left='off',right='off',bottom='off')\n",
        "    plt.show()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcXHdRv3tBYM"
      },
      "source": [
        "def policy_evaluation(env, policy, gamma = 1, theta = 1e-8, draw = False):\n",
        "  V = np.zeros(env.nS)\n",
        "  while True:\n",
        "    delta = 0\n",
        "    for s in range(env.nS):\n",
        "      Vs = 0\n",
        "      for a, action_prob in enumerate(policy[s]):\n",
        "        for prob, next_state, reward, done in env.P[s][a]:\n",
        "          Vs += action_prob * prob * (reward + gamma * V[next_state])\n",
        "      delta = max(delta, np.abs(Vs-V[s]))\n",
        "      V[s] = Vs\n",
        "    if draw: plot(V, policy, draw_vals = True)\n",
        "    if delta < theta:\n",
        "      break\n",
        "  return V"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwKhgEGyfYW3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0IS9bT-v-YY"
      },
      "source": [
        "policy = np.ones([env.nS, env.nA]) / env.nA"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO8tpl65wp4G"
      },
      "source": [
        "V = policy_evaluation(env, policy)\n",
        "print(V)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04UrFHUHAAI0"
      },
      "source": [
        "def q_from_v(env, V, s, gamma = 1):\n",
        "  q = np.zeros(env.nA)\n",
        "  for a in range(env.nA):\n",
        "    for prob, next_state, reward, done in env.P[s][a]:\n",
        "      q[a] += prob * (reward + gamma * V[next_state])\n",
        "  return q"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WF7MP4DfBdGf"
      },
      "source": [
        "def policy_improvement(env, V, gamma = 1):\n",
        "  policy = np.zeros([env.nS, env.nA]) / env.nA\n",
        "  for s in range(env.nS):\n",
        "    q = q_from_v(env, V, s, gamma)\n",
        "    best_a = np.argwhere(q==np.max(q)).flatten()\n",
        "    policy[s] = np.sum([np.eye(env.nA)[i] for i in best_a], axis=0)/len(best_a)\n",
        "  return policy\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vvcHtYyCuNk"
      },
      "source": [
        "def policy_iteration(env, gamma = 1, theta = 1e-8):\n",
        "  policy = np.ones([env.nS, env.nA]) / env.nA\n",
        "  while True:\n",
        "    V = policy_evaluation(env, policy, gamma, theta)\n",
        "    new_policy = policy_improvement(env, V)\n",
        "    if np.max(abs(policy_evaluation(env, policy) - policy_evaluation(env, new_policy))) < theta * 1e2:\n",
        "        break;\n",
        "    policy = copy.copy(new_policy)\n",
        "  return policy, V"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGB9v-IHfads"
      },
      "source": [
        "name = 'FrozenLake8x8-v0'\n",
        "env = gym.make(name, is_slippery=False)\n",
        "env.seed(742)\n",
        "env.action_space.seed(742)\n",
        "policy = np.ones([env.nS, env.nA]) / env.nA\n",
        "V = policy_evaluation(env,policy,draw=False)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9f8TPHcfg9q"
      },
      "source": [
        "policy_pi, V_pi = policy_iteration(env, gamma=0.7)\n",
        "plot(V_pi,policy_pi,1.0,draw_vals=True)\n",
        "plot(V_pi,policy_pi,1.0,draw_vals=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HZqL6TEgzJh"
      },
      "source": [
        "def value_iteration(env, gamma=1, theta=1e-8):\n",
        "    V = np.zeros(env.nS) # initial state value function\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in range(env.nS):\n",
        "            v_s = V[s] # store old value\n",
        "            q_s = q_from_v(env, V, s, gamma) # the action value function is calculated for all actions\n",
        "            V[s] = max(q_s) # the next value of the state function is the maximum of all action values\n",
        "            delta = max(delta, abs(V[s] - v_s))\n",
        "        if delta < theta: break\n",
        "    # lastly, at convergence, we can get a (optimal) policy from the optimal state value function\n",
        "    policy = policy_improvement(env, V, gamma)    \n",
        "    return policy, V"
      ],
      "execution_count": 64,
      "outputs": []
    }
  ]
}